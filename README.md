<p align="center"><a href="https://www.udacity.com/course/data-engineer-nanodegree--nd027">Udacity Data Engineer Nanodegree</a></p>

# Project 01: Data Modeling with Postgres

## Description
> In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## Datasets

### Song Dataset
This is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is a JSON containing metadata about a song.

Example:
```json
{ 
  "num_songs": 1, 
  "artist_id": "ARJIE2Y1187B994AB7", 
  "artist_latitude": null, 
  "artist_longitude": null, 
  "artist_location": "", 
  "artist_name": "Line Renaud", 
  "song_id": "SOUPIRU12A6D4FA1E1", 
  "title": "Der Kleine Dompfaff", 
  "duration": 152.92036, 
  "year": 0
 }
```

### Log Dataset
These are JSONs automatically generated by an [event simulator](https://github.com/Interana/eventsim) based on the songs above.

They are named after the date they allegedly occurred. Each one of them have multiples lines, and they can be read directly into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).

## Data Schema
|Name|Type|Description|Fields|
|--|--|--|--|
|songplays|fact|records in log data associated with song plays i.e. records with page `NextSong`|songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent|
|users|dimension|users in the app|user_id, first_name, last_name, gender, level|
|songs|dimension|songs in music database|song_id, title, artist_id, year, duration|
|artists|dimension|artists in music database|artist_id, name, location, latitude, longitude|
|time|dimension|timestamps of records in songplays broken down into specific units|start_time, hour, day, week, month, year, weekday|

## Files Included
```
data/..........................Folder containing raw data files
create_tables.py...............Script calling the queries and creating the tables
etl.ipynb......................Jupyter Notebook containing the test to the etl process
etl.py.........................Script containing all etl process code
Pipfile........................File listing requirements
Pipfile.lock...................Lock file with dependencies
README.md
sql_queries.py.................All queries to create, drop tables, insert data
test.ipynb.....................Jupyter Notebook with testing queries
```

## How to Run
Simply run `create_tables.py` to create the tables, and then `etl.py` to run the etl processes on the files inside the `data/` folder.

If you decide to use one of the `.ipynb` to explore, be aware that you will have to either restart the kernel or close the connection in order to be able to recreate the tables.

### Running Locally
This was tested in the Workspace environment provided by Udacity. If you are willing to run this locally, make sure to have [PostgreSQL](https://www.postgresql.org/) correctly installed and edit the connections to the database in all relevant places.

### Example Query
```
SELECT * FROM songplays where song_id IS NOT Null;
```

This query will return the only record in `songplays` with a _song_id_ and a _artist_id_.
